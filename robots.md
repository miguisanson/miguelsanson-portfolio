# This file tells search engine bots which pages they are allowed to crawl.
# 'User-agent: *' means this applies to all bots.
User-agent: *

# 'Allow: /' means that all content on the site is allowed to be crawled.
Allow: /

# You can disallow specific directories or files if needed.
# For example, to hide a private folder:
# Disallow: /private/

# Provide the path to your sitemap to help bots discover all your pages.
Sitemap: https://miguisanson.github.io/miguelsanson-portfolio/sitemap.xml
